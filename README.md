# Linear Genetic Programming (LGP) Framework

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Build Status](https://img.shields.io/badge/build-passing-green.svg)](#installation-and-build)
[![Language: C](https://img.shields.io/badge/language-C-blue.svg)](https://en.wikipedia.org/wiki/C_(programming_language))
[![Python API](https://img.shields.io/badge/Python-API-orange.svg)](lgp/README.md)

A high-performance framework for **Linear Genetic Programming (LGP)** implemented in C with a comprehensive Python interface. This system enables automatic program synthesis through evolutionary computation, representing programs as linear sequences of instructions operating on virtual registers.

## ðŸŽ¯ Overview

Linear Genetic Programming evolves computer programs represented as sequences of machine-like instructions. Unlike tree-based genetic programming, LGP operates on linear structures similar to assembly code, making it faster to execute and manipulate.

This framework provides:

- **High-Performance C Core**: Optimized implementation with OpenMP parallelization
- **Complete Python Interface**: Type-safe Python bindings with parameter validation
- **Cross-Platform Support**: Works on Linux, Windows, and macOS
- **Comprehensive Virtual Machine**: 87 operations including arithmetic, trigonometry, control flow
- **Rich Fitness Functions**: 25+ fitness functions for regression, classification, and robust optimization
- **Multiple Selection Methods**: 8+ selection algorithms including fitness sharing variants

## ðŸ—ï¸ Architecture

### Dual Interface Design

The framework provides two interfaces with different design philosophies:

#### C Interface (High Performance)
- **Zero Parameter Validation**: No runtime checks for maximum performance
- **Direct Memory Access**: Minimal overhead for fitness evaluations and evolution
- **OpenMP Parallelization**: Multi-threaded population evaluation
- **Manual Memory Management**: User responsible for proper resource management

#### Python Interface (Safety + Convenience)
- **Parameter Validation**: All inputs are validated before passing to C code
- **Automatic Memory Management**: ctypes handles resource cleanup
- **Type Safety**: Full type annotations and runtime type checking
- **NumPy Integration**: Direct array conversion and preprocessing
- **Unified Classes**: Direct inheritance from ctypes.Structure for zero-overhead C access

### Performance Philosophy

**C Interface**: Prioritizes raw speed over safety. No parameter validation or bounds checking is performed to maintain maximum performance during intensive evolutionary computations.

**Python Interface**: Provides a safety layer that validates all parameters, converts data types, and ensures proper formatting before delegation to the C core.

## ðŸš€ Key Features

### Core Engine (C)
- **87 Virtual Machine Operations**: Complete instruction set including arithmetic, trigonometry, logic, control flow
- **Modular Fitness System**: 25+ fitness functions with configurable parameters
- **Advanced Selection Methods**: Tournament, elitism, roulette wheel, and fitness sharing variants
- **Memory-Efficient Design**: Optimized data structures with SIMD alignment
- **OpenMP Parallelization**: Multi-threaded evolution with configurable thread count

### Python Interface
- **Unified Architecture**: Classes inherit directly from ctypes.Structure
- **Complete Type Safety**: Parameter validation and type checking
- **NumPy Integration**: Seamless array processing and conversion
- **Zero-Copy Operations**: Direct memory access to C structures
- **Comprehensive Documentation**: Complete API reference with examples

### Fitness Functions (25+)
- **Regression**: MSE, RMSE, MAE, RÂ², Pearson Correlation, MAPE
- **Classification**: Accuracy, F1-Score, Matthews Correlation, Cohen's Kappa
- **Robust Metrics**: Huber Loss, Pinball Loss, Worst Case Error
- **Penalized Functions**: Length/Clock penalized MSE for complexity control
- **Statistical**: Gaussian Log-Likelihood, Binary Cross-Entropy
- **Advanced**: Adversarial Perturbation Sensitivity, Conditional Value at Risk

### Selection Methods (8+)
- **Tournament Selection**: Standard and fitness sharing variants
- **Elitism**: Fixed size and percentage-based
- **Roulette Wheel**: Proportional selection with sampling
- **Fitness Sharing**: Diversity preservation mechanisms

## ðŸ“– Documentation

- **[C Interface Documentation](src/README.md)**: Complete C API reference and implementation details
- **[Python Interface Documentation](lgp/README.md)**: Python API guide with examples and best practices
- **[Examples](examples.py)**: Practical examples of regression, classification, and optimization problems

## ðŸ› ï¸ Installation and Build

### Prerequisites

```bash
# Ubuntu/Debian
sudo apt update
sudo apt install build-essential gcc make libomp-dev python3-dev

# CentOS/RHEL/Fedora  
sudo yum install gcc make libomp-devel python3-devel

# macOS with Homebrew
brew install gcc libomp make
```

### Compilation

```bash
# Build both C library and Python interface
make clean
make python

# C library only
make clean
make all

# Debug build with sanitizers
make clean
make DEBUG=1 python

# Optimized release build
make clean
CFLAGS="-O3 -march=native" make python
```

### Verification

```bash
# Test Python interface
python3 -c "
import lgp
print('âœ“ LGP Python interface loaded successfully')
print(f'âœ“ OpenMP threads available: {lgp.NUMBER_OF_OMP_THREADS}')

# Quick evolution test
import numpy as np
X = np.array([[1], [2], [3]], dtype=float)
y = np.array([2, 4, 6], dtype=float)
instruction_set = lgp.InstructionSet([lgp.Operation.ADD_F, lgp.Operation.MUL_F])
lgp_input = lgp.LGPInput.from_numpy(X, y, instruction_set)
population, _, _, _ = lgp.evolve(lgp_input, generations=1)
print('âœ“ Evolution test completed successfully')
"

# Run complete examples
python3 examples.py
```

## ðŸš€ Quick Start

### Python Usage (Recommended)

```python
import lgp
import numpy as np

# Generate dataset: f(x) = xÂ² + 2x + 1
X = np.random.uniform(-5, 5, (200, 1))
y = X[:, 0]**2 + 2*X[:, 0] + 1

# Define instruction set
instruction_set = lgp.InstructionSet([
    lgp.Operation.ADD_F, lgp.Operation.SUB_F, lgp.Operation.MUL_F,
    lgp.Operation.LOAD_RAM_F, lgp.Operation.MOV_F
])

# Create LGP input (with parameter validation)
lgp_input = lgp.LGPInput.from_numpy(X, y, instruction_set, ram_size=10)

# Run evolution
population, evals, gens, best_idx = lgp.evolve(
    lgp_input,
    fitness=lgp.MSE(),                    # Mean Squared Error
    selection=lgp.Tournament(3),          # Tournament selection
    init_params=(100, 5, 20),            # (pop_size, min_len, max_len)
    generations=50,
    verbose=1
)

# Results
best = population.get(best_idx)
print(f"Best fitness: {best.fitness}")
lgp.print_program(best)
```

### C Usage (Advanced)

```c
#include "evolution.h"
#include "genetics.h"
#include "fitness.h"

int main() {
    // Note: No parameter validation in C interface
    LGPInput input = create_lgp_input(data, rows, cols, instruction_set);
    
    LGPOptions opts = {
        .fitness = &MSE,
        .selection = &tournament,
        .generations = 100,
        .mutation_prob = 0.8,
        .crossover_prob = 0.95
    };
    
    LGPResult result = evolve(&input, &opts);
    
    printf("Best fitness: %f\n", result.best_fitness);
    print_program(&result.best_individual);
    
    return 0;
}
```

## ðŸ”§ Configuration

### Build Options

- `THREADS=N`: Set OpenMP thread count (default: 16)
- `DEBUG=1`: Enable debug build with sanitizers
- `CFLAGS="-O3 -march=native"`: Custom optimization flags

### Runtime Configuration

- **Python**: Automatic parameter validation and type conversion
- **C**: Manual parameter management for maximum performance
- **OpenMP**: Configurable thread count via `THREADS` environment variable

## ðŸ“Š Performance Characteristics

### C Interface
- **No Runtime Checks**: Maximum execution speed
- **Manual Memory Management**: User responsible for cleanup
- **Direct Structure Access**: Zero-overhead operations

### Python Interface  
- **Parameter Validation**: Input checking before C delegation
- **Automatic Memory Management**: ctypes handles cleanup
- **Type Conversion**: NumPy arrays automatically converted to C format

## ðŸ¤ Contributing

The framework is designed for extensibility:

1. **New Fitness Functions**: Add to `src/fitness.c` and corresponding Python wrapper
2. **New Operations**: Extend VM in `src/vm.c` and update Python enums
3. **New Selection Methods**: Implement in `src/selection.c` with Python interface

## ðŸ“ License

MIT License - see LICENSE file for details.

## ï¿½ References

Linear Genetic Programming: A comprehensive framework for evolving computer programs as linear sequences of instructions, combining the advantages of genetic programming with the efficiency of linear representations.

### Prerequisites
- **GCC 7+** or **Clang 10+** with OpenMP support
- **Make** for the build system
- **Python 3.7+** (optional, for Python interface)
- **libomp-dev** (for Clang on Ubuntu/Debian systems)

```bash
# Ubuntu/Debian - dependency installation
sudo apt update
sudo apt install build-essential gcc make libomp-dev python3-dev

# CentOS/RHEL/Fedora
sudo yum install gcc make libomp-devel python3-devel
# or
sudo dnf install gcc make libomp-devel python3-devel

# macOS with Homebrew
brew install gcc libomp make
```

### Compilation

```bash
# Clone and compile (if cloning from repository)
# cd LinearGeneticProgramming/sviluppi

# Complete build with Python interface
make clean
make python

# C library only (for C development)
make clean  
make all

# Debug build
make clean
make DEBUG=1 python

# Optimized build for release
make clean
CFLAGS="-O3 -march=native" make python
```

### Installation Verification

```bash
# Test C library (if main.c is compiled)
./LGP

# Test Python interface and thread support
python3 -c "
import lgp
print('âœ“ LGP Python interface loaded successfully')
print(f'âœ“ OpenMP threads available: {lgp.NUMBER_OF_OMP_THREADS}')
print('âœ“ Random number generators initialized automatically on import')

# Test PRNG functionality
lgp.random_init_all(42)  # Set custom seed
print('âœ“ Custom PRNG seeding works')

# Quick evolution test
import numpy as np
X = np.array([[1], [2], [3]], dtype=float)
y = np.array([2, 4, 6], dtype=float)
instruction_set = lgp.InstructionSet([lgp.Operation.ADD_F, lgp.Operation.MUL_F, lgp.Operation.LOAD_RAM_F])
lgp_input = lgp.LGPInput.from_numpy(X, y, instruction_set)
population, _, _, _ = lgp.evolve(lgp_input, generations=1)
print('âœ“ Evolution test completed successfully')
"

# Complete test with evolution
python3 examples.py
```

## ðŸš€ Quick Start

### C Usage

```c
#include "evolution.h"
#include "genetics.h"
#include "fitness.h"

int main() {
    // Initialization
    LGPInput input = create_lgp_input(data, rows, cols, instruction_set);
    
    // Parameter configuration
    LGPOptions opts = {
        .fitness = FITNESS_MSE,
        .selection = SELECTION_TOURNAMENT,
        .generations = 100,
        .mutation_prob = 0.8,
        .crossover_prob = 0.95
    };
    
    // Evolution
    LGPResult result = evolve(&input, &opts);
    
    // Results
    printf("Best fitness: %f\n", result.best_fitness);
    print_program(&result.best_individual);
    
    return 0;
}
```

### Python Usage

```python
import lgp
import numpy as np

# Dataset creation (function xÂ² + 2x + 1)
X = np.random.uniform(-5, 5, (200, 1))
y = X[:, 0]**2 + 2*X[:, 0] + 1

# Note: Random number generators are automatically initialized on import
# No need to call lgp.random_init_all() manually
print(f"Available OpenMP threads: {lgp.NUMBER_OF_OMP_THREADS}")

# Instruction set
instruction_set = lgp.InstructionSet([
    lgp.Operation.ADD_F, lgp.Operation.SUB_F, lgp.Operation.MUL_F,
    lgp.Operation.LOAD_RAM_F, lgp.Operation.MOV_F
])

# LGP Input
lgp_input = lgp.LGPInput.from_numpy(X, y, instruction_set, ram_size=10)

# Evolution
population, evals, gens, best_idx = lgp.evolve(
    lgp_input,
    fitness=lgp.MSE(),
    selection=lgp.Tournament(3),
    init_params=(100, 5, 20),
    generations=50,
    verbose=1
)

# Results
best = population.get(best_idx)
print(f"Best fitness: {best.fitness}")
lgp.print_program(best)
```

## ï¿½ Random Number Generation and Seeding

### Automatic Initialization
The LGP framework automatically initializes all random number generators on import:

```python
import lgp  # Automatically calls random_init_all(0) for all threads
```

**Default behavior:**
- **Seed**: 0 (deterministic but different from system random)
- **Thread Safety**: Each OpenMP thread gets its own PRNG state
- **Reproducibility**: Same seed always produces identical results

### Custom Seeding for Reproducibility

```python
import lgp

# Set a specific seed for reproducible experiments
lgp.random_init_all(12345)

# Now all subsequent evolution runs will be deterministic
population, _, _, best_idx = lgp.evolve(...)
```

### Thread-specific Seeding (Advanced)

```python
import lgp

# Initialize specific thread (useful for debugging)
thread_id = 0
seed = 54321
lgp.random_init(thread_id, seed)

# Check number of available threads
print(f"Available OpenMP threads: {lgp.NUMBER_OF_OMP_THREADS}")
```

### Reproducibility Best Practices

```python
import lgp
import numpy as np

# 1. Set LGP seed for evolutionary algorithm
lgp.random_init_all(42)

# 2. Set NumPy seed for data generation
np.random.seed(42)

# 3. Generate dataset
X = np.random.uniform(-5, 5, (200, 1))
y = X[:, 0]**2 + 2*X[:, 0] + 1

# 4. Run evolution - results will be perfectly reproducible
population, _, _, best_idx = lgp.evolve(
    lgp.LGPInput.from_numpy(X, y, instruction_set),
    fitness=lgp.MSE(),
    selection=lgp.Tournament(3),
    init_params=(100, 5, 20),
    generations=50
)

# This exact sequence will always produce the same results
```

### Multi-run Experiments

```python
import lgp
import numpy as np

# Run multiple independent experiments
results = []
for run in range(10):
    # Different seed for each run
    lgp.random_init_all(run + 1000)
    
    # Evolution
    population, _, _, best_idx = lgp.evolve(...)
    best_fitness = population.get(best_idx).fitness
    results.append(best_fitness)

# Statistical analysis
mean_fitness = np.mean(results)
std_fitness = np.std(results)
print(f"Mean fitness: {mean_fitness:.6f} Â± {std_fitness:.6f}")
```

### PRNG Notes
- **Thread Safety**: The framework uses MT19937 generators, one per OpenMP thread
- **Performance**: PRNG operations are highly optimized and add minimal overhead
- **Initialization**: Automatic initialization ensures no uninitialized state issues
- **Compatibility**: Works identically on all supported platforms (Linux, Windows, macOS)

## ï¿½ðŸ—ï¸ System Architecture

### Directory Structure
```
LinearGeneticProgramming/sviluppi/
â”œâ”€â”€ src/                    # Core C implementation
â”‚   â”œâ”€â”€ vm.{c,h}           # Virtual Machine and operations
â”‚   â”œâ”€â”€ genetics.{c,h}     # Genetic structures and population
â”‚   â”œâ”€â”€ evolution.{c,h}    # Main evolutionary algorithm
â”‚   â”œâ”€â”€ fitness.{c,h}      # Fitness functions
â”‚   â”œâ”€â”€ selection.{c,h}    # Selection methods
â”‚   â”œâ”€â”€ creation.{c,h}     # Population initialization
â”‚   â””â”€â”€ main.c             # C application entry point
â”œâ”€â”€ lgp/                   # Python interface
â”‚   â”œâ”€â”€ __init__.py        # Public API
â”‚   â”œâ”€â”€ base.py            # Library loading and base types
â”‚   â”œâ”€â”€ vm.py              # VM wrapper and Operation enum
â”‚   â”œâ”€â”€ genetics.py        # Genetic classes (Individual, Population)
â”‚   â”œâ”€â”€ evolution.py       # Main evolve() function
â”‚   â”œâ”€â”€ fitness.py         # Python fitness classes
â”‚   â”œâ”€â”€ selection.py       # Python selection classes
â”‚   â”œâ”€â”€ creation.py        # Initialization methods
â”‚   â”œâ”€â”€ utils.py           # Utility functions
â”‚   â””â”€â”€ README.md          # Python interface documentation
â”œâ”€â”€ bin/                   # Compiled object files
â”œâ”€â”€ examples.py            # Complete Python usage examples
â”œâ”€â”€ Makefile              # Build system
â”œâ”€â”€ liblgp.so             # Compiled shared library
â””â”€â”€ README.md             # This file
```

### Core Components

#### Virtual Machine (vm.{c,h})
- **87 Operations**: Complete instruction set for computing
- **4 Float Registers**: For floating-point operations
- **4 Integer Registers**: For integer and logical operations
- **Flag Register**: For conditional operations and flow control
- **RAM/ROM Memory**: Access to data and working memory

#### Genetic Algorithm (genetics.{c,h}, evolution.{c,h})
- **Linear Representation**: Programs as instruction sequences
- **Multi-point Crossover**: Code segment exchange between parents
- **Adaptive Mutation**: Instruction insertion, deletion, modification
- **Bloat Control**: Prevention of uncontrolled program growth

#### Fitness System (fitness.{c,h})
- **Efficient Evaluation**: Program execution on optimized VM
- **Multiple Metrics**: Support for regression, classification, custom
- **Penalizations**: Automatic complexity control (length, clock)

#### Selection Methods (selection.{c,h})
- **Tournament Selection**: With configurable parameters
- **Elitism**: Preservation of best solutions
- **Fitness Sharing**: Genetic diversity maintenance
- **Roulette Wheel**: Fitness-proportional selection

## ðŸŽ¯ Use Cases

### Symbolic Regression
```python
# Automatic discovery of mathematical formulas
import lgp
import numpy as np

# Dataset with hidden nonlinear relationship
X = np.random.uniform(-3, 3, (300, 2))
y = np.sin(X[:, 0]) * np.exp(X[:, 1]/2) + np.random.normal(0, 0.05, 300)

# Instruction set with advanced mathematical functions
instruction_set = lgp.InstructionSet([
    lgp.Operation.ADD_F, lgp.Operation.SUB_F, lgp.Operation.MUL_F, lgp.Operation.DIV_F,
    lgp.Operation.SIN, lgp.Operation.COS, lgp.Operation.EXP, lgp.Operation.LN,
    lgp.Operation.POW, lgp.Operation.SQRT, lgp.Operation.LOAD_RAM_F, lgp.Operation.MOV_F
])

lgp_input = lgp.LGPInput.from_numpy(X, y, instruction_set, ram_size=15)

# Evolution optimized for formula discovery
population, _, _, best_idx = lgp.evolve(
    lgp_input,
    fitness=lgp.RSquared(),  # Maximize RÂ²
    selection=lgp.FitnessSharingTournament(4, 0.1),  # Diversity to avoid local minima
    init_params=(200, 10, 40),  # Large population, complex programs
    generations=150,
    mutation_prob=0.85,
    verbose=1
)

best = population.get(best_idx)
print(f"RÂ² discovered: {1.0 - best.fitness:.4f}")
lgp.print_program(best)  # Discovered formula in assembly-like format
```

### Classification with Automatic Feature Engineering
```python
from sklearn.datasets import load_breast_cancer
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler

# Load real dataset
data = load_breast_cancer()
X_train, X_test, y_train, y_test = train_test_split(
    data.data, data.target, test_size=0.3, random_state=42
)

# Normalization
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test = scaler.transform(X_test)

# LGP for feature engineering + classification
instruction_set = lgp.InstructionSet([
    lgp.Operation.ADD_F, lgp.Operation.SUB_F, lgp.Operation.MUL_F, lgp.Operation.DIV_F,
    lgp.Operation.SQRT, lgp.Operation.POW, lgp.Operation.SIN, lgp.Operation.COS,
    lgp.Operation.CMOV_L_F, lgp.Operation.CMOV_G_F,  # For conditional decisions
    lgp.Operation.LOAD_RAM_F, lgp.Operation.STORE_RAM_F, lgp.Operation.MOV_F
])

lgp_input = lgp.LGPInput.from_numpy(X_train, y_train, instruction_set, ram_size=20)

# Multi-objective fitness: accuracy + simplicity
population, _, _, best_idx = lgp.evolve(
    lgp_input,
    fitness=lgp.F1Score(),  # Optimize F1-score for balanced classes
    selection=lgp.Tournament(5),
    init_params=(300, 15, 50),
    target=0.98,  # Target F1-score
    generations=200,
    verbose=1
)

best_classifier = population.get(best_idx)
print(f"Training F1-score: {best_classifier.fitness:.4f}")

# Test set evaluation (simplified - would require complete implementation)
print("Evolved classifier program:")
lgp.print_program(best_classifier)
```

### Multi-Objective Optimization
```python
# Automatic balance between accuracy and complexity
instruction_set = lgp.InstructionSet([
    lgp.Operation.ADD_F, lgp.Operation.SUB_F, lgp.Operation.MUL_F,
    lgp.Operation.LOAD_RAM_F, lgp.Operation.MOV_F
])

lgp_input = lgp.LGPInput.from_numpy(X, y, instruction_set, ram_size=8)

# Fitness that penalizes long programs
population, _, _, best_idx = lgp.evolve(
    lgp_input,
    fitness=lgp.LengthPenalizedMSE(penalty=0.01),  # MSE + 0.01 * program_length
    selection=lgp.FitnessSharingPercentualElitism(0.1, 0.15),  # Maintains diversity
    init_params=(150, 5, 25),
    max_individ_len=20,  # Force short programs
    generations=100,
    verbose=1
)

best = population.get(best_idx)
print(f"Length-penalized fitness: {best.fitness:.6f}")
print(f"Program length: {best.size} instructions")

# Calculate pure MSE for comparison
pure_mse = lgp.MSE().evaluate_individual(best, lgp_input)
print(f"Pure MSE: {pure_mse:.6f}")
print(f"Length penalty: {best.fitness - pure_mse:.6f}")
```

## ðŸ”§ Extensions and Customization

### Adding New VM Operations

To add custom operations to the Virtual Machine:

1. **Modify `src/vm.h`**:
```c
typedef enum {
    // ... existing operations ...
    OP_CUSTOM_FUNCTION = 87,
} OperationCode;
```

2. **Implement in `src/vm.c`**:
```c
case OP_CUSTOM_FUNCTION:
    vm->core.freg[instr->reg[0]] = custom_mathematical_function(
        vm->core.freg[instr->reg[1]], 
        vm->core.freg[instr->reg[2]]
    );
    break;
```

3. **Add to Python `lgp/vm.py`**:
```python
class Operation(Enum):
    # ... existing operations ...
    CUSTOM_FUNCTION = OperationWrapper(name=b"CUSTOM_FUNCTION", regs=3, addr=0, state_changer=0, code=87)
```

### Creating Custom Fitness Functions

```python
class CustomRobustFitness(lgp.FitnessAssessment):
    def __init__(self, outlier_threshold=2.0, complexity_penalty=0.001):
        super().__init__()
        self.outlier_threshold = outlier_threshold
        self.complexity_penalty = complexity_penalty
    
    def evaluate_individual(self, individual, lgp_input):
        predictions = []
        for i in range(lgp_input.rows):
            output = individual.execute(lgp_input, i)
            predictions.append(output[0])
        
        predictions = np.array(predictions)
        targets = np.array([lgp_input.get_target(i) for i in range(lgp_input.rows)])
        
        # Robust loss function (Huber-like)
        errors = np.abs(predictions - targets)
        robust_errors = np.where(
            errors <= self.outlier_threshold,
            0.5 * errors**2,  # Quadratic for small errors
            self.outlier_threshold * errors - 0.5 * self.outlier_threshold**2  # Linear for outliers
        )
        
        base_fitness = np.mean(robust_errors)
        complexity_penalty = self.complexity_penalty * individual.size
        
        return base_fitness + complexity_penalty
    
    @property
    def c_wrapper(self):
        # Use MSE as fallback for C compatibility
        return lgp.MSE().c_wrapper

# Usage
custom_fitness = CustomRobustFitness(outlier_threshold=1.5, complexity_penalty=0.002)
population, _, _, _ = lgp.evolve(lgp_input, fitness=custom_fitness)
```

### Pipeline di Preprocessing

```python
class LGPPipeline:
    def __init__(self, instruction_set, preprocessors=None, postprocessors=None):
        self.instruction_set = instruction_set
        self.preprocessors = preprocessors or []
        self.postprocessors = postprocessors or []
        self.scaler = None
        self.best_individual = None
    
    def add_preprocessor(self, func):
        self.preprocessors.append(func)
        return self
    
    def add_postprocessor(self, func):
        self.postprocessors.append(func)
        return self
    
    def fit(self, X, y, **evolution_kwargs):
        # Preprocessing
        X_processed, y_processed = X.copy(), y.copy()
        for preprocessor in self.preprocessors:
            X_processed, y_processed = preprocessor(X_processed, y_processed)
        
        # Automatic normalization
        from sklearn.preprocessing import StandardScaler
        self.scaler = StandardScaler()
        X_processed = self.scaler.fit_transform(X_processed)
        
        # LGP Evolution
        lgp_input = lgp.LGPInput.from_numpy(X_processed, y_processed, self.instruction_set)
        population, _, _, best_idx = lgp.evolve(lgp_input, **evolution_kwargs)
        self.best_individual = population.get(best_idx)
        self.lgp_input = lgp_input
        
        return self
    
    def predict(self, X):
        # Apply preprocessing pipeline
        X_processed = X.copy()
        for preprocessor in self.preprocessors:
            X_processed, _ = preprocessor(X_processed, None)
        
        X_processed = self.scaler.transform(X_processed)
        
        # Predict using evolved program
        predictions = []
        for i in range(len(X_processed)):
            # Note: this is a simplification, 
            # a complete implementation would require C extension
            output = self.best_individual.execute(self.lgp_input, i % self.lgp_input.rows)
            predictions.append(output[0])
        
        # Apply postprocessing
        predictions = np.array(predictions)
        for postprocessor in self.postprocessors:
            predictions = postprocessor(predictions)
        
        return predictions

# Usage
def add_polynomial_features(X, y):
    X_poly = np.column_stack([X, X**2, X**3])
    return X_poly, y

def apply_sigmoid(predictions):
    return 1 / (1 + np.exp(-predictions))

pipeline = (LGPPipeline(instruction_set)
           .add_preprocessor(add_polynomial_features)
           .add_postprocessor(apply_sigmoid))

pipeline.fit(X_train, y_train, generations=100)
predictions = pipeline.predict(X_test)
```

## ðŸ› Troubleshooting

### Compilation Issues

**Error**: `fatal error: omp.h: No such file or directory`
```bash
# Ubuntu/Debian
sudo apt install libomp-dev

# CentOS/RHEL
sudo yum install libomp-devel

# macOS
brew install libomp
```

**Error**: `undefined reference to 'omp_get_thread_num'`
```bash
# Add explicit linking flag
LDFLAGS="-fopenmp" make python
```

### Runtime Issues

**Error**: `ImportError: No module named 'lgp'`
```bash
# Make sure you are in the correct directory
cd /path/to/LinearGeneticProgramming/sviluppi
python3 -c "import lgp"

# Add to PYTHONPATH if necessary
export PYTHONPATH=$PYTHONPATH:$(pwd)
```

**Error**: `OSError: liblgp.so: cannot open shared object file`
```bash
# Verify that the library has been compiled
ls -la liblgp.so

# Recompile if necessary
make clean && make python

# Add to LD_LIBRARY_PATH if necessary (usually not needed)
export LD_LIBRARY_PATH=$LD_LIBRARY_PATH:$(pwd)
```

**Segmentation faults or infinite loops**:
These issues have been resolved in the current version through:
- Automatic PRNG initialization on import (`lgp.random_init_all(0)`)
- Proper struct initialization matching C zero-initialization
- Thread-safe random number generation for all OpenMP threads

If you still encounter issues:
```python
# Explicitly reinitialize PRNGs
import lgp
lgp.random_init_all(42)  # Try with a different seed

# Check thread count
print(f"Threads: {lgp.NUMBER_OF_OMP_THREADS}")

# Run a minimal test
lgp_input = lgp.LGPInput.from_numpy(X, y, instruction_set)
population, _, _, _ = lgp.evolve(lgp_input, generations=1)  # Single generation test
```

**Non-reproducible results**:
```python
# Ensure both LGP and NumPy seeds are set
import lgp
import numpy as np

lgp.random_init_all(42)    # LGP evolution
np.random.seed(42)         # Data generation

# Results should now be perfectly reproducible
```

### Performance Issues

**Very slow evolution**:
- Reduce `max_clock` for faster programs
- Use smaller instruction set
- Reduce population size for initial tests
- Compile with optimization flags: `CFLAGS="-O3" make python`

**Premature convergence**:
- Use fitness sharing: `lgp.FitnessSharingTournament()`
- Increase mutation probability
- Increase initial diversity with larger population

## ðŸ“Š Performance e Benchmarking

### Benchmark on Standard Hardware
**System**: Intel i7-8700K, 16GB RAM, GCC 9.4.0

| Problem | Dataset Size | Population | Generations | Time | Final Fitness |
|----------|--------------|-------------|-------------|--------|---------------|
| Polynomial Regression | 500 samples | 100 | 50 | 12.3s | 0.001245 MSE |
| Classification | 1000 samples | 200 | 100 | 45.7s | 0.924 Accuracy |
| Symbolic Regression | 300 samples | 150 | 150 | 67.2s | 0.89 RÂ² |

### Compilation Optimizations

```bash
# Optimized release build
CFLAGS="-O3 -march=native -flto" make python

# Build with profiling
CFLAGS="-O2 -pg" make python
# Run benchmark, then:
gprof ./bin/main gmon.out > profile.txt

# Parallel build
make -j$(nproc) python
```

## ðŸ“œ License and Contributions

This project is released under the MIT license. Contributions, bug reports and feature requests are welcome.

### How to Contribute
1. Fork the repository
2. Create feature branch (`git checkout -b feature/amazing-feature`)
3. Commit changes (`git commit -m 'Add amazing feature'`)
4. Push to branch (`git push origin feature/amazing-feature`)
5. Open Pull Request

### Roadmap
- [ ] GPU computing support (CUDA/OpenCL)
- [ ] Evolved model serialization/deserialization
- [ ] Integration with scikit-learn pipeline
- [ ] Interactive evolution visualization
- [ ] Multi-output programming support
- [ ] Automatic hyperparameter optimization

---

**Authors**: LGP Development Team  
**Version**: 1.0.0  
**Documentation updated**: July 6, 2025  
**PRNG System**: Automatic initialization with MT19937, thread-safe, seed 0 default

For technical support or questions: Create an issue in the project repository
